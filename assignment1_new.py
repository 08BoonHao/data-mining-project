# -*- coding: utf-8 -*-
"""Assignment1 - new.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FH_9fjiyHxbOVCDXF5vwB11lSjtc53ds
"""

pip install scikit-learn-intelex --progress-bar off

from sklearnex import patch_sklearn
patch_sklearn()

# import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from scipy import stats
from sklearn.calibration import CalibratedClassifierCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, auc
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC, LinearSVC
from sklearn.model_selection import RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn.naive_bayes import GaussianNB
from scipy.stats import uniform
from imblearn.over_sampling import SMOTE
from sklearn.neural_network import MLPClassifier

import warnings
warnings.filterwarnings("ignore")

# read the data from the excel sheet
data = pd.read_csv('((GAssign) BankLoanApproval.csv')

"""Step 1: Data **Exploration**"""

# print the first 5 rows of the data to briefly know the structure for each column
data.head()

# print this to know the number of rows and columns in the data as well as the data type for each column
data.info()

# print this to know the descriptive statistics for each variable except for categorical variable
data.describe()

# print this to know the descriptive satistics for other variables
data.describe(include=['O'])

# print the countplot for the education column against the default column
sns.countplot(data, x='Education', hue='Default')

# print the countplot for the employment column against the default column
sns.countplot(data, x='EmploymentType', hue='Default')

# print the countplot for the marital status column against the default column
sns.countplot(data, x='MaritalStatus', hue='Default')

# print the countplot for the HasMortgage column against the default column
sns.countplot(data, x='HasMortgage', hue='Default')

# print the countplot for the HasDependents column against the default column
sns.countplot(data, x='HasDependents', hue='Default')

# print the countplot for the LoanPurpose column against the default column
sns.countplot(data, x='LoanPurpose', hue='Default')

# print the countplot for the HasCoSigner column against the default column
sns.countplot(data, x='HasCoSigner', hue='Default')

# print the histplot for age column against the default column
sns.histplot(data, x='Age', hue='Default', bins=10, multiple="stack")

# print the histplot for income column against the default column
sns.histplot(data, x='Income', hue='Default', bins=10, multiple="stack")

# print the histplot for loan amount column against the default column
sns.histplot(data, x='LoanAmount', hue='Default', bins=10, multiple="stack")

# print the histplot for credit score column against the default column
sns.histplot(data, x='CreditScore', hue='Default', bins=10, multiple="stack")

# print the histplot for months employed column against the default column
sns.histplot(data, x='MonthsEmployed', hue='Default', bins=10, multiple="stack")

# print the histplot for NumCreditLine column against the default column
sns.histplot(data, x='NumCreditLines', hue='Default', bins=4, multiple="stack")

# print the histplot for interest rate column against the default column
sns.histplot(data, x='InterestRate', hue='Default', bins=10, multiple="stack")

# print the histplot for loan term column against the default column
sns.histplot(data, x='LoanTerm', hue='Default', bins=5, multiple="stack")

# print the histplot for DTIRatio column against the default column
sns.histplot(data, x='DTIRatio', hue='Default', bins=10, multiple="stack")

# print a pie chart for the Default column to see whether the class is imbalanced or not
temp = data['Default'].value_counts()
plt.pie(temp.values, labels=temp.index, autopct='%1.1f%%')
plt.show()

# Define feature vector and target variable (class)
X = data.drop(['Default'], axis=1)
y = data['Default']

# deleting of unneeded columns since this columns will not help us in making a predictive model
X.drop(['LoanID'], axis=1, inplace=True)

# Transforming the qualitatives variables into quantitatives for training data
X['Education'] = X['Education'].map({"Bachelor's": 0, "High School": 1, "Master's": 2, "PhD": 3} ).astype(int)
X['EmploymentType'] = X['EmploymentType'].map( {'Full-time': 0, 'Part-time': 1,'Unemployed':2, 'Self-employed': 3} ).astype(int)
X['MaritalStatus'] = X['MaritalStatus'].map( {'Divorced': 0, 'Married': 1, 'Single': 2}).astype(int)
X['HasMortgage'] = X['HasMortgage'].map( {'No': 0, 'Yes': 1}).astype(int)
X['HasDependents'] = X['HasDependents'].map( {'No': 0, 'Yes': 1}).astype(int)
X['LoanPurpose'] = X['LoanPurpose'].map( {'Auto': 0, 'Business': 1, 'Education': 2, 'Home': 3, 'Other': 4}).astype(int)
X['HasCoSigner'] = X['HasCoSigner'].map( {'No': 0, 'Yes': 1}).astype(int)
X.head()

# Set up the oversampling method
oversampler = SMOTE()
X_new, y_new = oversampler.fit_resample(X, y)

X.shape, X_new.shape

"""Step 2: Data Splitting (60% for training set, 20% for testing set, and 20% for validation set)"""

#Split data into separate training, test, and validation set
X_train, X_temp, y_train, y_temp = train_test_split(X_new, y_new, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

#Check the shape of X_train and X_test
print("train sample size: ", X_train.shape)
print("test sample size: ", X_test.shape)
print("validation sample size: ", X_val.shape)

"""Step 3: Data Preprocessing

Training Data
"""

# Looking for null values in training data
print("Null values:\n", X_train.isnull().sum())

# Check duplicate rows in training data
duplicate_rows = X_train[X_train.duplicated()]
print("Number of duplicate rows: ", duplicate_rows.shape)

# Removing outliers using z-score
z = np.abs(stats.zscore(X_train))

df3 = X_train[(z<3).all(axis=1)]
df3.shape

df3['Default'] = y_train

# Finding the correlation between variables
pearsonCorr = df3.corr(method='pearson')

# Create mask for both correlation matrices
# Pearson corr masking
# Generating mask for upper triangle
maskP = np.triu(np.ones_like(pearsonCorr,dtype=bool))

# Adjust mask and correlation
maskP = maskP[1:,:-1]
pCorr = pearsonCorr.iloc[1:,:-1].copy()
# Setting up a diverging palette
cmap = sns.diverging_palette(0, 200, 150, 50, as_cmap=True)
fig = plt.subplots(figsize=(20,9))
sns.heatmap(pCorr, vmin=-1,vmax=1, cmap = cmap, annot=True, linewidth=0.3, mask=maskP)
plt.title("Pearson Correlation")

"""Testing Data"""

# Looking for null values in testing data
print("Null values:\n", X_test.isnull().sum())

# Check duplicate rows in testing data
duplicate_rows = X_test[X_test.duplicated()]
print("Number of duplicate rows: ", duplicate_rows.shape)

"""Validation Data"""

# Looking for null values in validation data
print("Null values:\n", X_val.isnull().sum())

# Check duplicate rows in validation data
duplicate_rows = X_val[X_val.duplicated()]
print("Number of duplicate rows: ", duplicate_rows.shape)

"""Step 4: Algorithm and Selection, Step 5: Model Training, Step 6: Model Evaluation, Step 7: Comparison and Selection"""

# Scale the features using StandardScaler
scaler = StandardScaler()
X_train_new = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
X_val = scaler.transform(X_val)

"""Logistic Regression (before tuning the parameters)"""

logReg = LogisticRegression(random_state=0, solver='liblinear')
logReg.fit(X_train_new, y_train)

y_pred_logReg = logReg.predict(X_test)
y_pred_logReg_val = logReg.predict(X_val)

# Classification Report for LR with validation data
print("\nLogistic Regression Evaluation:\n")
print(classification_report(y_val, y_pred_logReg_val))

# Confusion Matrix for LR with validation data
print("\nLogistic Regression confusion matrix:\n")
cm = confusion_matrix(y_val, y_pred_logReg_val)
print(cm)

# Classification Report for LR with test data
print("\nLogistic Regression Evaluation:\n")
print(classification_report(y_test, y_pred_logReg))

# Confusion Matrix for LR with test data
print("\nLogistic Regression confusion matrix:\n")
cm = confusion_matrix(y_test, y_pred_logReg)
print(cm)

"""Logistic Regression (after tuning the parameters)"""

param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],
              'penalty': ['l1', 'l2']}
grid_search = GridSearchCV(LogisticRegression(random_state=0, solver='liblinear'), param_grid, cv=5)
grid_search.fit(X_train_new, y_train)

print("Best Parameters: ", grid_search.best_params_)

y_pred_logReg_hyper = grid_search.predict(X_test)
y_pred_logReg_hyper_val = grid_search.predict(X_val)

# Classification Report for LR with validation data
print("\nLogistic Regression Evaluation:\n")
print(classification_report(y_val, y_pred_logReg_hyper_val))

# Confusion Matrix for LR with validation data
print("\nLogistic Regression confusion matrix:\n")
cm = confusion_matrix(y_val, y_pred_logReg_hyper_val)
print(cm)

# Classification Report for LR with test data
print("\nLogistic Regression Evaluation:\n")
print(classification_report(y_test, y_pred_logReg_hyper))

# Confusion Matrix for LR with test data
print("\nLogistic Regression confusion matrix:\n")
cm = confusion_matrix(y_test, y_pred_logReg_hyper)
print(cm)

"""Decision Trees (before tuning the parameters)"""

dt = DecisionTreeClassifier(criterion='entropy', max_depth=5)
dt = dt.fit(X_train_new, y_train)
y_pred_dt = dt.predict(X_test)
y_pred_dt_val = dt.predict(X_val)

# Classification Report for Decision Trees with validation data
print("\nDecision Trees Evaluation:\n")
print(classification_report(y_val, y_pred_dt_val))

# Confusion Matrix for Decision Trees with validation data
print("\nDecision Trees confusion matrix:\n")
cm = confusion_matrix(y_val, y_pred_dt_val)
print(cm)

# Classification Report for Decision Trees with testing data
print("\nDecision Trees Evaluation:\n")
print(classification_report(y_test, y_pred_dt))

# Confusion Matrix for Decision Trees with validation data
print("\nDecision Trees confusion matrix:\n")
cm = confusion_matrix(y_test, y_pred_dt)
print(cm)

"""Decision Trees (after tuning the parameters)"""

# Define the parameter grid to search
param_grid = {
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize DecisionTreeClassifier
dt = DecisionTreeClassifier()

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5)

# Fit the GridSearchCV to the training data
grid_search.fit(X_train_new, y_train)

# Get the best parameters
best_params = grid_search.best_params_
print("Best Parameters:", best_params)

# Retrieve the best model
best_dt = grid_search.best_estimator_

y_pred_dt_hyper = best_dt.predict(X_test)
y_pred_dt_hyper_val = best_dt.predict(X_val)

# Classification Report for Decision Trees with validation data
print("\nDecision Trees Evaluation:\n")
print(classification_report(y_val, y_pred_dt_hyper_val))

# Confusion Matrix for Decision Trees with validation data
print("\nDecision Trees confusion matrix:\n")
cm = confusion_matrix(y_val, y_pred_dt_hyper_val)
print(cm)

# Classification Report for Decision Trees with validation data
print("\nDecision Trees Evaluation:\n")
print(classification_report(y_test, y_pred_dt_hyper))

# Confusion Matrix for Decision Trees with validation data
print("\nDecision Trees confusion matrix:\n")
cm = confusion_matrix(y_test, y_pred_dt_hyper)
print(cm)

"""Random Forest Classifier (before tuning the parameters)"""

rf = RandomForestClassifier()
rf.fit(X_train_new,y_train)
y_pred_rf = rf.predict(X_test)
y_pred_rf_val = rf.predict(X_val)

# Classification Report for Random Forest Classifier with validation data
print("\nRandom Forest Classifier Evaluation:\n")
print(classification_report(y_val, y_pred_rf_val))

# Confusion Matrix for Random Forest Classifier with validation data
print("\nRandom Forest Classifier confusion matrix:\n")
cm = confusion_matrix(y_val, y_pred_rf_val)
print(cm)

# Classification Report for Random Forest Classifier with testing data
print("\nRandom Forest Classifier Evaluation:\n")
print(classification_report(y_test, y_pred_rf))

# Confusion Matrix for Random Forest Classifier with testing data
print("\nRandom Forest Classifier confusion matrix:\n")
cm = confusion_matrix(y_test, y_pred_rf)
print(cm)

"""Random Forest Classifier (after tuning the parameters)"""

rf=RandomForestClassifier()
grid_space={
              'min_samples_leaf':[1,2,3],
              'min_samples_split':[1,2,3]
           }

grid = GridSearchCV(rf,param_grid=grid_space,cv=3,scoring='accuracy')
model_grid = grid.fit(X_train_new, y_train)

# Get the best parameters
best_params = model_grid.best_params_
print("Best Parameters:", best_params)

# Retrieve the best model
best_rf = model_grid.best_estimator_

y_pred_rf_hyper = best_rf.predict(X_test)
y_pred_rf_hyper_val = best_rf.predict(X_val)

# Classification Report for Random Forest Classifier with validation data
print("\nRandom Forest Classifier Evaluation:\n")
print(classification_report(y_val, y_pred_rf_hyper_val))

# Confusion Matrix for Random Forest Classifier with validation data
print("\nRandom Forest Classifier confusion matrix:\n")
cm = confusion_matrix(y_val, y_pred_rf_hyper_val)
print(cm)

# Classification Report for Random Forest Classifier with testing data
print("\nRandom Forest Classifier Evaluation:\n")
print(classification_report(y_test, y_pred_rf_hyper))

# Confusion Matrix for Random Forest Classifier with testing data
print("\nRandom Forest Classifier confusion matrix:\n")
cm = confusion_matrix(y_test, y_pred_rf_hyper)
print(cm)

"""Naive Bayes(before tuning the hyper parameter)"""

nb = GaussianNB()
nb.fit(X_train_new,y_train)

y_pred_nb = nb.predict(X_test)
y_pred_nb_val = nb.predict(X_val)

# Classification Report for Naive Bayes with validation data
print("\nNaive Bayes Classifier Evaluation:\n")
print(classification_report(y_val, y_pred_nb_val))

# Confusion Matrix for Naive Bayes with validation data
print("\nNaive Bayes confusion matrix:\n")
cm = confusion_matrix(y_val, y_pred_nb_val)
print(cm)

# Classification Report for Naive Bayes with testing data
print("\nNaive Bayes Classifier Evaluation:\n")
print(classification_report(y_test, y_pred_nb))

# Confusion Matrix for Naive Bayes with testing data
print("\nNaive Bayes confusion matrix:\n")
cm = confusion_matrix(y_test, y_pred_nb)
print(cm)

"""Naive Bayes (after tuning the hyperparameter)"""

nb = GaussianNB()

params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}
gs_NB = GridSearchCV(estimator=nb,
                 param_grid=params_NB,
                 cv=10,   # use any cross validation technique
                 verbose=1,
                 scoring='accuracy')
gs_NB.fit(X_train_new, y_train)

gs_NB.best_params_

y_pred_nb_hyper = gs_NB.predict(X_test)
y_pred_nb_hyper_val = gs_NB.predict(X_val)

# Classification Report for Naive Bayes with validation data
print("\nNaive Bayes Classifier Evaluation:\n")
print(classification_report(y_val, y_pred_nb_hyper_val))

# Confusion Matrix for Naive Bayes with validation data
print("\nNaive Bayes confusion matrix:\n")
cm = confusion_matrix(y_val, y_pred_nb_hyper_val)
print(cm)

# Classification Report for Naive Bayes with testing data
print("\nNaive Bayes Classifier Evaluation:\n")
print(classification_report(y_test, y_pred_nb_hyper))

# Confusion Matrix for Naive Bayes with testing data
print("\nNaive Bayes confusion matrix:\n")
cm = confusion_matrix(y_test, y_pred_nb_hyper)
print(cm)

"""Neural Network (before tuning the hyperparameter)"""

mlp = MLPClassifier()
mlp.fit(X_train_new,y_train)

y_pred_mlp = mlp.predict(X_test)
y_pred_mlp_val = mlp.predict(X_val)

# Classification Report for Neural Network with validation data
print("\nNeural Network Evaluation:\n")
print(classification_report(y_val, y_pred_mlp_val))

# Confusion Matrix for Neural Network with validation data
print("\nNeural Network confusion matrix:\n")
cm = confusion_matrix(y_val, y_pred_mlp_val)
print(cm)

# Classification Report for Neural Network with testing data
print("\nNeural Network Evaluation:\n")
print(classification_report(y_test, y_pred_mlp))

# Confusion Matrix for Neural Network with testing data
print("\nNeural Network confusion matrix:\n")
cm = confusion_matrix(y_test, y_pred_mlp)
print(cm)

"""Neural Network (after tuning the hyperparameter)"""

mlp = MLPClassifier()
tuned_parameters= {'hidden_layer_sizes': range(1,200,10),
                   'activation': ['tanh','logistic','relu'], 'alpha':[0.0001,0.001,0.01,0.1,1,10],
                   'max_iter': range(50,200,50)}

model_mlp= RandomizedSearchCV(mlp, tuned_parameters,cv=3,scoring='accuracy',n_iter=3,n_jobs= -1,random_state=5)
model_mlp.fit(X_train_new, y_train)

model_mlp.best_params_

y_pred_mlp_hyper = model_mlp.predict(X_test)
y_pred_mlp_hyper_val = model_mlp.predict(X_val)

# Classification Report for Neural Network with validation data
print("\nNeural Network Evaluation:\n")
print(classification_report(y_val, y_pred_mlp_hyper_val))

# Confusion Matrix for Neural Network with validation data
print("\nNeural Network confusion matrix:\n")
cm = confusion_matrix(y_val, y_pred_mlp_hyper_val)
print(cm)

# Classification Report for Neural Network with testing data
print("\nNeural Network Evaluation:\n")
print(classification_report(y_test, y_pred_mlp_hyper))

# Confusion Matrix for Neural Network with testing data
print("\nNeural Network confusion matrix:\n")
cm = confusion_matrix(y_test, y_pred_mlp_hyper)
print(cm)

"""K-Nearest Neighbors"""

# find the accuracy and errors rate for k starts from 5 to 11
k_values = [i for i in range (5,11)]
scores = []
error_rate = []
for k in k_values:
 knn = KNeighborsClassifier(n_neighbors=k)
 knn.fit(X_train_new, y_train)
 y_pred = knn.predict(X_test)
 scores.append(knn.score(X_test, y_test))
 error_rate.append(np.mean(y_pred!=y_test))

# Plot the accuracy score for each k-values
sns.lineplot(x = k_values, y = scores, marker = 'o')
plt.title("K-Nearest Neighbors (KNN)")
plt.xlabel("K Values")
plt.ylabel("Accuracy Score")
plt.show()

# Plot the error rate for each k-values
sns.lineplot(x = k_values, y = error_rate, marker = 'o')
plt.title("K-Nearest Neighbors (KNN)")
plt.xlabel("K Values")
plt.ylabel("Error rate")
plt.show()

#Get the best k and train kNN
best_index = np.argmax(scores)
best_k = k_values[best_index]
print("\nk = ", best_k)
knn = KNeighborsClassifier(n_neighbors=best_k)
knn.fit(X_train_new, y_train)
y_pred_knn = knn.predict(X_test)
y_pred_knn_val = knn.predict(X_val)

# Classification Report for K-Nearest Neighbour with validation data
print("\nK-Nearest Neighbour Evaluation:\n")
print(classification_report(y_val, y_pred_knn_val))

# Confusion Matrix for K-Nearest Neighbour with validation data
print("\nK-Nearest Neighbour confusion matrix:\n")
cm = confusion_matrix(y_val, y_pred_knn_val)
print(cm)

# Classification Report for K-Nearest Neighbour with testing data
print("\nK-Nearest Neighbour Evaluation:\n")
print(classification_report(y_test, y_pred_knn))

# Confusion Matrix for K-Nearest Neighbour with testing data
print("\nK-Nearest Neighbour confusion matrix:\n")
cm = confusion_matrix(y_test, y_pred_knn)
print(cm)

"""Support Vector Machine (SVM)"""

svm = CalibratedClassifierCV(LinearSVC())
svm.fit(X_train_new, y_train)
y_pred_svm = svm.predict(X_test)
y_pred_svm_val = svm.predict(X_val)

# Classification Report for Support Vector Machine with validation data
print("\nSupport Vector Machine Evaluation:\n")
print(classification_report(y_val, y_pred_svm_val))

# Confusion Matrix for Support Vector Machine with validation data
print("\nSupport Vector Machine confusion matrix:\n")
cm = confusion_matrix(y_val, y_pred_svm_val)
print(cm)

# Classification Report for Support Vector Machine with testing data
print("\nSupport Vector Machine Evaluation:\n")
print(classification_report(y_test, y_pred_svm))

# Confusion Matrix for Support Vector Machine with testing data
print("\nSupport Vector Machine confusion matrix:\n")
cm = confusion_matrix(y_test, y_pred_svm)
print(cm)

"""AUC Score"""

auc_score_logReg = roc_auc_score(y_test, logReg.predict_proba(X_test)[:, 1])
auc_score_dt = roc_auc_score(y_test, grid_search.predict_proba(X_test)[:, 1])
auc_score_rf = roc_auc_score(y_test, model_grid.predict_proba(X_test)[:, 1])
auc_score_nb = roc_auc_score(y_test, gs_NB.predict_proba(X_test)[:, 1])
auc_score_mlp = roc_auc_score(y_test, model_mlp.predict_proba(X_test)[:, 1])
auc_score_knn = roc_auc_score(y_test, knn.predict_proba(X_test)[:, 1])
auc_score_svm = roc_auc_score(y_test, svm.predict_proba(X_test)[:, 1])
print("AUC score for LR: ", auc_score_logReg)
print("AUC score for Decision Trees: ", auc_score_dt)
print("AUC score for Random Forest Classifier: ", auc_score_rf)
print("AUC score for Naive Bayes: ", auc_score_nb)
print("AUC score for Neural Network: ", auc_score_mlp)
print("AUC score for K-Nearest Neighbour: ", auc_score_knn)
print("AUC score for Support Vector Machine: ", auc_score_svm)

fpr_LR, tpr_LR, thresholds_LR = roc_curve(y_test, logReg.predict_proba(X_test)[:,1])
fpr_DT, tpr_DT, thresholds_DT = roc_curve(y_test, grid_search.predict_proba(X_test)[:,1])
fpr_RF, tpr_RF, thresholds_RF = roc_curve(y_test, model_grid.predict_proba(X_test)[:,1])
fpr_NB, tpr_NB, thresholds_NB = roc_curve(y_test, gs_NB.predict_proba(X_test)[:,1])
fpr_MLP, tpr_MLP, thresholds_MLP = roc_curve(y_test, model_mlp.predict_proba(X_test)[:,1])
fpr_KNN, tpr_KNN, thresholds_KNN = roc_curve(y_test, knn.predict_proba(X_test)[:,1])
fpr_SVM, tpr_SVM, thresholds_SVM = roc_curve(y_test, svm.predict_proba(X_test)[:,1])
plt.plot(fpr_LR, tpr_LR, linestyle = "--", color = "red", label = "LR")
plt.plot(fpr_DT, tpr_DT, linestyle = "--", color = "orange", label = "DT")
plt.plot(fpr_RF, tpr_RF, linestyle = "--", color = "yellow", label = "RF")
plt.plot(fpr_NB, tpr_NB, linestyle = "--", color = "green", label = "NB")
plt.plot(fpr_MLP, tpr_MLP, linestyle = "--", color = "blue", label = "MLP")
plt.plot(fpr_KNN, tpr_KNN, linestyle = "--", color = "purple", label = "KNN")
plt.plot(fpr_SVM, tpr_SVM, linestyle = "--", color = "black", label = "SVM")
plt.title('Receiver Operator Characteristics (ROC)')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive rate')
plt.legend(loc = 'best')
plt.savefig('ROC', dpi = 300)
plt.show()

"""Step 8: Storing the Model"""

import joblib

joblib.dump(rf, 'loan_application_predict')

"""Step 9: Loading the Model"""

model = joblib.load('loan_application_predict')

"""Step 10: Interacting with the Model"""

new_applicant = pd.read_csv('((GAssign) NewApplicants.csv')

new_applicant.head()

new_applicant.info()

application = new_applicant.drop(['Default'], axis=1)
application_new = application.drop(['LoanID'], axis=1)

application_new['Education'] = application_new['Education'].map({"Bachelor's": 0, "High School": 1, "Master's": 2, "PhD": 3} ).astype(int)
application_new['EmploymentType'] = application_new['EmploymentType'].map( {'Full-time': 0, 'Part-time': 1,'Unemployed':2, 'Self-employed': 3} ).astype(int)
application_new['MaritalStatus'] = application_new['MaritalStatus'].map( {'Divorced': 0, 'Married': 1, 'Single': 2}).astype(int)
application_new['HasMortgage'] = application_new['HasMortgage'].map( {'No': 0, 'Yes': 1}).astype(int)
application_new['HasDependents'] = application_new['HasDependents'].map( {'No': 0, 'Yes': 1}).astype(int)
application_new['LoanPurpose'] = application_new['LoanPurpose'].map( {'Auto': 0, 'Business': 1, 'Education': 2, 'Home': 3, 'Other': 4}).astype(int)
application_new['HasCoSigner'] = application_new['HasCoSigner'].map( {'No': 0, 'Yes': 1}).astype(int)
application_new.head()

application_scale = scaler.fit_transform(application_new)

result = model.predict(application_scale)

result